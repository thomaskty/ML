{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMZmJWhnLcxRIXH9WPYtBQh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. Cluster Shapes: Convex and Isotropic\n","\n","### Convex Clusters\n","\n","- A cluster is **convex** if any two points within the cluster can be connected by a straight line that remains entirely inside the cluster.\n","- Convexity aligns well with algorithms like **K-Means**, which rely on Euclidean distance.\n","\n","### Isotropic Clusters\n","\n","- Clusters are **isotropic** if they spread out uniformly in all directions, meaning:\n","    - The data has equal variance along all directions.\n","    - The covariance matrix is proportional to the identity matrix.\n","\n","For isotropic clusters, the covariance matrix is:\n","\n","**Σ = λI**,\n","\n","where **λ** is a scalar, and **I** is the identity matrix.\n","\n","### Non-Isotropic Clusters\n","\n","- Variance is unequal in different directions.\n","- Covariance exists between features.\n"],"metadata":{"id":"scv-0fXR5uoM"}},{"cell_type":"code","source":["import numpy as np\n","X = np.array([[2, 3], [4, 6], [6, 9]])\n","\n","mean_X = np.mean(X, axis=0)\n","centered_X = X - mean_X\n","cov_matrix = np.cov(centered_X, rowvar=False)\n","print(\"Covariance Matrix:\", cov_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nltX2pbU5xKL","executionInfo":{"status":"ok","timestamp":1735666932802,"user_tz":-330,"elapsed":828,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"fd3813d5-c81e-4d12-d11b-c94b5f7572cb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Covariance Matrix: [[4. 6.]\n"," [6. 9.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CeVn14rM54Vi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## 3. Eigenvalues, Eigenvectors, and Principal Components\n","\n","The **eigenvalues** and **eigenvectors** of the covariance matrix provide:\n","\n","- **Eigenvectors**: Directions of maximum variance (principal directions).\n","- **Eigenvalues**: Magnitudes of variance along those directions.\n","\n","- The eigenvector with the largest eigenvalue points in the direction of the greatest spread.\n","- Smaller eigenvalues correspond to less significant directions.\n","\n","### Proof: Principal Directions Maximize Variance\n","\n","The eigenvectors of $\\Sigma$ maximize the variance of the projected data. For a unit vector $\\mathbf{w}$:\n","$\\text{Variance of projection} = \\mathbf{w}^T \\Sigma \\mathbf{w}$\n","Maximizing this variance under the constraint $\\| \\mathbf{w} \\| = 1$ leads to the eigenvalue equation:\n","$\\Sigma \\mathbf{w} = \\lambda \\mathbf{w}$\n","\n"],"metadata":{"id":"VfrzGuDi59-f"}},{"cell_type":"code","source":["eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n","print(\"Eigenvalues:\", eig_values)\n","print(\"Eigenvectors:\", eig_vectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvbgint354Ns","executionInfo":{"status":"ok","timestamp":1734208893245,"user_tz":-330,"elapsed":382,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"c9f9bb24-66b3-49a2-b58e-5a12037ce903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eigenvalues: [ 0. 13.]\n","Eigenvectors: [[-0.83205029 -0.5547002 ]\n"," [ 0.5547002  -0.83205029]]\n"]}]},{"cell_type":"markdown","source":["## 4. Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality reduction technique that uses eigenvalues and eigenvectors to project data onto its most informative directions.\n","\n","1. Compute the covariance matrix $\\Sigma$.\n","2. Perform eigenvalue decomposition.\n","3. Select the top $k$ eigenvectors corresponding to the largest eigenvalues.\n","4. Project the data onto these eigenvectors.\n"],"metadata":{"id":"CVUYrHjo6IYI"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Fit PCA\n","pca = PCA(n_components=2)\n","pca.fit(X)\n","\n","# Transform the data\n","X_pca = pca.transform(X)\n","print(\"PCA Components:\", pca.components_)\n","print(\"Explained Variance:\", pca.explained_variance_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekAnTbYr6EX3","executionInfo":{"status":"ok","timestamp":1734208943008,"user_tz":-330,"elapsed":8092,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"1e9638d8-e4b3-4c66-8c88-daa58dde4ae0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PCA Components: [[ 0.5547002   0.83205029]\n"," [ 0.83205029 -0.5547002 ]]\n","Explained Variance: [13.  0.]\n"]}]},{"cell_type":"markdown","source":["## 5. Transforming Data Using Covariance Matrix\n","\n","If you multiply data rows by the covariance matrix, the data gets stretched or compressed along the eigenvector directions:\n","$\\mathbf{x'} = \\Sigma \\mathbf{x}$\n","\n","- **Stretching**: The data is scaled along the eigenvectors by the eigenvalues.\n","- **Rotation**: The data aligns with the principal directions.\n"],"metadata":{"id":"gqfvDM_E6Ma-"}},{"cell_type":"code","source":["# Transform data using the covariance matrix\n","transformed_data = np.dot(X, cov_matrix)\n","print(\"Transformed Data:\", transformed_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UxTZQ80W6NH-","executionInfo":{"status":"ok","timestamp":1734208986123,"user_tz":-330,"elapsed":430,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"a7cf3732-3788-4e6e-8d45-9ea510fc866b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed Data: [[ 26.  39.]\n"," [ 52.  78.]\n"," [ 78. 117.]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the covariance matrix (Sigma)\n","Sigma = np.array([[4, 6],\n","                  [6, 9]])\n","\n","# Define the vector x\n","x = np.array([2, 3])\n","\n","# Perform the multiplication of x by Sigma\n","x_prime = np.dot(Sigma, x)\n","\n","# Display the result\n","print(\"Transformed vector x':\", x_prime)\n","\n","# Compute eigenvalues and eigenvectors of Sigma\n","eigenvalues, eigenvectors = np.linalg.eig(Sigma)\n","\n","# Display the eigenvalues and eigenvectors\n","print(\"Eigenvalues:\", eigenvalues)\n","print(\"Eigenvectors:\\n\", eigenvectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzUbBhbb6hTK","executionInfo":{"status":"ok","timestamp":1734209055855,"user_tz":-330,"elapsed":427,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"1e0a0027-047e-44cb-925a-5843322dd52f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed vector x': [26 39]\n","Eigenvalues: [ 0. 13.]\n","Eigenvectors:\n"," [[-0.83205029 -0.5547002 ]\n"," [ 0.5547002  -0.83205029]]\n"]}]},{"cell_type":"markdown","source":["## 7. Summary\n","\n","- **Convexity** ensures that clusters are geometrically simple and compatible with algorithms like K-Means.\n","- **Isotropy** describes uniform spread in all directions, often associated with spherical clusters.\n","- The **covariance matrix** encodes the relationships between features and determines the shape of data clusters.\n","- **Eigenvalues and eigenvectors** of the covariance matrix define the principal directions and variances of the data.\n","- Transforming data with the covariance matrix scales and rotates it along the principal directions.\n"],"metadata":{"id":"2_Fjukju5c29"}},{"cell_type":"code","source":[],"metadata":{"id":"kjHvw7uP5hC3"},"execution_count":null,"outputs":[]}]}